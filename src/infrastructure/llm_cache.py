"""\nSistema de caché para resultados de LLM.\n\nImplementa un sistema de caché persistente y en memoria para\nresultados de refinamiento LLM, reduciendo llamadas a APIs externas\ny mejorando el rendimiento general del sistema.\n"""\nimport hashlib\nimport json\nimport os\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom typing import Dict, Optional, Any\n\nclass LLMCache:\n    def __init__(self, cache_dir: Optional[Path] = None):\n        """\n        Inicializa el sistema de caché LLM.\n        \n        Args:\n            cache_dir: Directorio para almacenar caché persistente.\n                      Si es None, usa 'data/cache/llm'.\n        """\n        self.cache_dir = cache_dir or Path('data/cache/llm')\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.memory_cache: Dict[str, str] = {}\n    \n    def _generate_hash(self, text: str, model: str, temperature: float) -> str:\n        """\n        Genera un hash único para una solicitud LLM.\n        \n        Args:\n            text: Texto de entrada\n            model: Modelo LLM utilizado\n            temperature: Temperatura de generación\n            \n        Returns:\n            Hash único para esta combinación de parámetros\n        """\n        # Crear una representación única de los parámetros\n        key_parts = [\n            text.strip(),\n            str(model),\n            str(temperature)\n        ]\n        key_str = "|".join(key_parts)\n        return hashlib.md5(key_str.encode('utf-8')).hexdigest()\n    \n    def get(self, text: str, model: str, temperature: float = 0.0) -> Optional[str]:\n        """\n        Obtiene un resultado cacheado para una solicitud LLM.\n        \n        Args:\n            text: Texto de entrada\n            model: Modelo LLM utilizado\n            temperature: Temperatura de generación\n            \n        Returns:\n            Resultado cacheado o None si no existe\n        """\n        key = self._generate_hash(text, model, temperature)\n        \n        # Primero buscar en memoria\n        if key in self.memory_cache:\n            return self.memory_cache[key]\n            \n        # Luego buscar en disco\n        cache_file = self.cache_dir / f"{key}.json"\n        if cache_file.exists():\n            try:\n                with open(cache_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    self.memory_cache[key] = data['content']  # Actualizar caché en memoria\n                    return data['content']\n            except (json.JSONDecodeError, KeyError, IOError):\n                return None\n                \n        return None\n    \n    def set(self, text: str, model: str, temperature: float, result: str) -> None:\n        """\n        Almacena un resultado en la caché.\n        \n        Args:\n            text: Texto de entrada\n            model: Modelo LLM utilizado\n            temperature: Temperatura de generación\n            result: Resultado a almacenar\n        """\n        key = self._generate_hash(text, model, temperature)\n        \n        # Guardar en memoria\n        self.memory_cache[key] = result\n        \n        # Guardar en disco\n        cache_file = self.cache_dir / f"{key}.json"\n        data = {\n            'content': result,\n            'params': {\n                'model': model,\n                'temperature': temperature,\n                'text_hash': hashlib.md5(text.encode('utf-8')).hexdigest()  # Solo guardar hash del texto\n            },\n            'timestamp': os.path.getmtime(cache_file) if cache_file.exists() else None\n        }\n        \n        try:\n            with open(cache_file, 'w', encoding='utf-8') as f:\n                json.dump(data, f, ensure_ascii=False)\n        except IOError:\n            pass  # Fallar silenciosamente si no se puede escribir\n    \n    def invalidate(self, text: str, model: str, temperature: float = 0.0) -> None:\n        """\n        Invalida una entrada específica de la caché.\n        \n        Args:\n            text: Texto de entrada\n            model: Modelo LLM utilizado\n            temperature: Temperatura de generación\n        """\n        key = self._generate_hash(text, model, temperature)\n        \n        # Eliminar de memoria\n        if key in self.memory_cache:\n            del self.memory_cache[key]\n            \n        # Eliminar de disco\n        cache_file = self.cache_dir / f"{key}.json"\n        if cache_file.exists():\n            try:\n                cache_file.unlink()\n            except IOError:\n                pass  # Fallar silenciosamente\n    \n    def clear(self) -> None:\n        """Limpia toda la caché (memoria y disco)."""\n        # Limpiar memoria\n        self.memory_cache.clear()\n        \n        # Limpiar disco\n        try:\n            for cache_file in self.cache_dir.glob("*.json"):\n                cache_file.unlink()\n        except (IOError, OSError):\n            pass  # Fallar silenciosamente\n